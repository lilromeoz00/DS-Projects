{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":29,"outputs":[{"output_type":"stream","text":"/kaggle/input/company-bankruptcy-prediction/data.csv\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import BatchNormalization, Dense, Dropout\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nfrom sklearn import metrics   \nfrom sklearn.model_selection import KFold","execution_count":30,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/company-bankruptcy-prediction/data.csv')\nY = df['Bankrupt?']\nX = df.drop(['Bankrupt?'], axis=1)\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state = 42)","execution_count":31,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inputs = np.concatenate((X_train, X_test), axis=0)\ntargets = np.concatenate((Y_train, Y_test), axis=0)\n\nacc_per_fold = []\nloss_per_fold = []\n\nfold_no = 1\nnum_folds = 10\n\nkfold = KFold(n_splits=num_folds, shuffle=True)\n\n# K-fold Cross Validation model evaluation\nfor train, test in kfold.split(inputs, targets):\n\n    # Cool model stuff (Did some off notebook tweaking to get the best model!)\n    model = Sequential()\n\n    model.add(Dense(64, activation='relu', input_shape=(95,)))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.25))\n\n    model.add(Dense(64, activation='relu')) \n    model.add(BatchNormalization())\n    model.add(Dropout(0.25))\n\n    model.add(Dense(1, activation='sigmoid'))\n    \n\n    checkpoint = ModelCheckpoint('BankruptcyModel.h5',\n                             monitor='val_loss',\n                             mode='min',\n                             save_best_only=True,\n                             verbose=1)\n    earlystop = EarlyStopping(monitor='val_loss',\n                          min_delta=0,\n                          patience=25,\n                          verbose=1,\n                          restore_best_weights=True\n                          )\n    reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n                              factor=0.2,\n                              patience=3,\n                              verbose=1,\n                              min_delta=0.0001)\n    # This will help with accuracy\n    callbacks = [earlystop,checkpoint,reduce_lr]\n    \n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n    # Asthetics\n    print('------------------------------------------------------------------------')\n    print(f'Training Fold {fold_no} / {num_folds}')\n\n    history = model.fit(inputs[train], targets[train],\n              epochs=25,\n              callbacks = callbacks,\n              verbose=1)\n\n    # Generalization metrics\n    scores = model.evaluate(inputs[test], targets[test], verbose=1)\n    \n    print(f'Score for Fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n    acc_per_fold.append(scores[1] * 100)\n    loss_per_fold.append(scores[0])\n\n    fold_no = fold_no + 1","execution_count":32,"outputs":[{"output_type":"stream","text":"------------------------------------------------------------------------\nTraining Fold 1 / 10\nEpoch 1/25\n192/192 [==============================] - 1s 2ms/step - loss: 0.6495 - accuracy: 0.6789\nEpoch 2/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.2251 - accuracy: 0.9541\nEpoch 3/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1597 - accuracy: 0.9648\nEpoch 4/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1612 - accuracy: 0.9645\nEpoch 5/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1550 - accuracy: 0.9637\nEpoch 6/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1436 - accuracy: 0.9640\nEpoch 7/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1477 - accuracy: 0.9659\nEpoch 8/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1346 - accuracy: 0.9681\nEpoch 9/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1229 - accuracy: 0.9727\nEpoch 10/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1309 - accuracy: 0.9679\nEpoch 11/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1472 - accuracy: 0.9645\nEpoch 12/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1366 - accuracy: 0.9655\nEpoch 13/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1308 - accuracy: 0.9659\nEpoch 14/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1280 - accuracy: 0.9676\nEpoch 15/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1243 - accuracy: 0.9689\nEpoch 16/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1297 - accuracy: 0.9675\nEpoch 17/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1256 - accuracy: 0.9689\nEpoch 18/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1386 - accuracy: 0.9626\nEpoch 19/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1241 - accuracy: 0.9674\nEpoch 20/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1276 - accuracy: 0.9677\nEpoch 21/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1226 - accuracy: 0.9678\nEpoch 22/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1175 - accuracy: 0.9695\nEpoch 23/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1310 - accuracy: 0.9647\nEpoch 24/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1250 - accuracy: 0.9670\nEpoch 25/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1256 - accuracy: 0.9677\n22/22 [==============================] - 0s 1ms/step - loss: 0.1147 - accuracy: 0.9736\nScore for Fold 1: loss of 0.11468835920095444; accuracy of 97.3607063293457%\n------------------------------------------------------------------------\nTraining Fold 2 / 10\nEpoch 1/25\n192/192 [==============================] - 1s 2ms/step - loss: 0.6683 - accuracy: 0.6573\nEpoch 2/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.2231 - accuracy: 0.9538\nEpoch 3/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1575 - accuracy: 0.9644\nEpoch 4/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1473 - accuracy: 0.9667\nEpoch 5/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1604 - accuracy: 0.9640\nEpoch 6/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1254 - accuracy: 0.9705\nEpoch 7/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1463 - accuracy: 0.9635\nEpoch 8/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1330 - accuracy: 0.9699\nEpoch 9/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1348 - accuracy: 0.9660\nEpoch 10/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1338 - accuracy: 0.9673\nEpoch 11/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1281 - accuracy: 0.9692\nEpoch 12/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1340 - accuracy: 0.9676\nEpoch 13/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1362 - accuracy: 0.9656\nEpoch 14/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1168 - accuracy: 0.9703\nEpoch 15/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1188 - accuracy: 0.9701\nEpoch 16/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1155 - accuracy: 0.9719\nEpoch 17/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1255 - accuracy: 0.9693\nEpoch 18/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1244 - accuracy: 0.9700\nEpoch 19/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1132 - accuracy: 0.9714\nEpoch 20/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1222 - accuracy: 0.9669\nEpoch 21/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1258 - accuracy: 0.9682\nEpoch 22/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1293 - accuracy: 0.9663\nEpoch 23/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1236 - accuracy: 0.9678\nEpoch 24/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1206 - accuracy: 0.9686\nEpoch 25/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1158 - accuracy: 0.9696\n22/22 [==============================] - 0s 1ms/step - loss: 0.1434 - accuracy: 0.9677\nScore for Fold 2: loss of 0.14338241517543793; accuracy of 96.77419066429138%\n------------------------------------------------------------------------\nTraining Fold 3 / 10\nEpoch 1/25\n192/192 [==============================] - 1s 2ms/step - loss: 0.6621 - accuracy: 0.6717\nEpoch 2/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.2289 - accuracy: 0.9477\nEpoch 3/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1662 - accuracy: 0.9638\nEpoch 4/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1609 - accuracy: 0.9632\nEpoch 5/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1544 - accuracy: 0.9627\nEpoch 6/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1306 - accuracy: 0.9701\nEpoch 7/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1440 - accuracy: 0.9642\nEpoch 8/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1448 - accuracy: 0.9654\nEpoch 9/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1366 - accuracy: 0.9675\nEpoch 10/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1437 - accuracy: 0.9645\nEpoch 11/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1299 - accuracy: 0.9686\nEpoch 12/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1267 - accuracy: 0.9711\nEpoch 13/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1276 - accuracy: 0.9694\nEpoch 14/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1318 - accuracy: 0.9663\nEpoch 15/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1275 - accuracy: 0.9700\nEpoch 16/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1284 - accuracy: 0.9685\nEpoch 17/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1433 - accuracy: 0.9650\nEpoch 18/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1303 - accuracy: 0.9678\nEpoch 19/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1176 - accuracy: 0.9694\nEpoch 20/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1201 - accuracy: 0.9715\nEpoch 21/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1180 - accuracy: 0.9710\nEpoch 22/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1260 - accuracy: 0.9678\nEpoch 23/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1308 - accuracy: 0.9657\nEpoch 24/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1136 - accuracy: 0.9714\nEpoch 25/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1115 - accuracy: 0.9717\n","name":"stdout"},{"output_type":"stream","text":"22/22 [==============================] - 0s 966us/step - loss: 0.1136 - accuracy: 0.9707\nScore for Fold 3: loss of 0.11359000205993652; accuracy of 97.06745147705078%\n------------------------------------------------------------------------\nTraining Fold 4 / 10\nEpoch 1/25\n192/192 [==============================] - 1s 2ms/step - loss: 0.6542 - accuracy: 0.6537\nEpoch 2/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.2354 - accuracy: 0.9491\nEpoch 3/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1538 - accuracy: 0.9661\nEpoch 4/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1531 - accuracy: 0.9653\nEpoch 5/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1453 - accuracy: 0.9673\nEpoch 6/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1429 - accuracy: 0.9657\nEpoch 7/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1502 - accuracy: 0.9652\nEpoch 8/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1386 - accuracy: 0.9684\nEpoch 9/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1421 - accuracy: 0.9639\nEpoch 10/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1324 - accuracy: 0.9686\nEpoch 11/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1402 - accuracy: 0.9657\nEpoch 12/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1334 - accuracy: 0.9673\nEpoch 13/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1276 - accuracy: 0.9681\nEpoch 14/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1361 - accuracy: 0.9670\nEpoch 15/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1214 - accuracy: 0.9692\nEpoch 16/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1394 - accuracy: 0.9669\nEpoch 17/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1294 - accuracy: 0.9671\nEpoch 18/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1248 - accuracy: 0.9679\nEpoch 19/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1460 - accuracy: 0.9633\nEpoch 20/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1246 - accuracy: 0.9690\nEpoch 21/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1226 - accuracy: 0.9689\nEpoch 22/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1348 - accuracy: 0.9665\nEpoch 23/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1318 - accuracy: 0.9650\nEpoch 24/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1123 - accuracy: 0.9715\nEpoch 25/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1240 - accuracy: 0.9673\n22/22 [==============================] - 0s 1ms/step - loss: 0.1322 - accuracy: 0.9663\nScore for Fold 4: loss of 0.13220366835594177; accuracy of 96.62756323814392%\n------------------------------------------------------------------------\nTraining Fold 5 / 10\nEpoch 1/25\n192/192 [==============================] - 1s 2ms/step - loss: 0.6553 - accuracy: 0.6582\nEpoch 2/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.2211 - accuracy: 0.9530\nEpoch 3/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1516 - accuracy: 0.9677\nEpoch 4/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1473 - accuracy: 0.9670\nEpoch 5/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1410 - accuracy: 0.9687\nEpoch 6/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1445 - accuracy: 0.9674\nEpoch 7/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1468 - accuracy: 0.9653\nEpoch 8/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1382 - accuracy: 0.9683\nEpoch 9/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1412 - accuracy: 0.9682\nEpoch 10/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1350 - accuracy: 0.9696\nEpoch 11/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1433 - accuracy: 0.9643\nEpoch 12/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1226 - accuracy: 0.9703\nEpoch 13/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1376 - accuracy: 0.9674\nEpoch 14/25\n192/192 [==============================] - ETA: 0s - loss: 0.1159 - accuracy: 0.97 - 0s 2ms/step - loss: 0.1177 - accuracy: 0.9702\nEpoch 15/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1232 - accuracy: 0.9699\nEpoch 16/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1306 - accuracy: 0.9687\nEpoch 17/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1248 - accuracy: 0.9691\nEpoch 18/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1222 - accuracy: 0.9698\nEpoch 19/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1130 - accuracy: 0.9721\nEpoch 20/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1247 - accuracy: 0.9701\nEpoch 21/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1159 - accuracy: 0.9708\nEpoch 22/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1198 - accuracy: 0.9702\nEpoch 23/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1266 - accuracy: 0.9679\nEpoch 24/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1348 - accuracy: 0.9656\nEpoch 25/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1336 - accuracy: 0.9662\n22/22 [==============================] - 0s 995us/step - loss: 0.1437 - accuracy: 0.9619\nScore for Fold 5: loss of 0.14365985989570618; accuracy of 96.18768095970154%\n------------------------------------------------------------------------\nTraining Fold 6 / 10\nEpoch 1/25\n192/192 [==============================] - 1s 2ms/step - loss: 0.6769 - accuracy: 0.6448\nEpoch 2/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.2349 - accuracy: 0.9500\nEpoch 3/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1600 - accuracy: 0.9685\nEpoch 4/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1473 - accuracy: 0.9666\nEpoch 5/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1357 - accuracy: 0.9692\nEpoch 6/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1408 - accuracy: 0.9676\nEpoch 7/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1462 - accuracy: 0.9642\nEpoch 8/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1325 - accuracy: 0.9682\nEpoch 9/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1293 - accuracy: 0.9698\nEpoch 10/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1362 - accuracy: 0.9671\nEpoch 11/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1261 - accuracy: 0.9697\nEpoch 12/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1329 - accuracy: 0.9689\nEpoch 13/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1239 - accuracy: 0.9695\nEpoch 14/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1205 - accuracy: 0.9711\nEpoch 15/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1306 - accuracy: 0.9674\nEpoch 16/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1248 - accuracy: 0.9675\nEpoch 17/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1263 - accuracy: 0.9700\nEpoch 18/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1174 - accuracy: 0.9709\nEpoch 19/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1340 - accuracy: 0.9674\nEpoch 20/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1324 - accuracy: 0.9670\nEpoch 21/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1292 - accuracy: 0.9673\nEpoch 22/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1327 - accuracy: 0.9660\nEpoch 23/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1245 - accuracy: 0.9684\nEpoch 24/25\n","name":"stdout"},{"output_type":"stream","text":"192/192 [==============================] - 0s 2ms/step - loss: 0.1162 - accuracy: 0.9697\nEpoch 25/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1238 - accuracy: 0.9688\n22/22 [==============================] - 0s 1ms/step - loss: 0.1404 - accuracy: 0.9633\nScore for Fold 6: loss of 0.14040081202983856; accuracy of 96.334308385849%\n------------------------------------------------------------------------\nTraining Fold 7 / 10\nEpoch 1/25\n192/192 [==============================] - 1s 2ms/step - loss: 0.6587 - accuracy: 0.6685\nEpoch 2/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.2224 - accuracy: 0.9474\nEpoch 3/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1610 - accuracy: 0.9635\nEpoch 4/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1636 - accuracy: 0.9623\nEpoch 5/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1259 - accuracy: 0.9701\nEpoch 6/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1400 - accuracy: 0.9660\nEpoch 7/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1297 - accuracy: 0.9708\nEpoch 8/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1310 - accuracy: 0.9683\nEpoch 9/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1295 - accuracy: 0.9690\nEpoch 10/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1301 - accuracy: 0.9679\nEpoch 11/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1332 - accuracy: 0.9686\nEpoch 12/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1189 - accuracy: 0.9713\nEpoch 13/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1300 - accuracy: 0.9663\nEpoch 14/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1330 - accuracy: 0.9673\nEpoch 15/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1359 - accuracy: 0.9671\nEpoch 16/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1132 - accuracy: 0.9704\nEpoch 17/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1152 - accuracy: 0.9706\nEpoch 18/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1224 - accuracy: 0.9675\nEpoch 19/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1111 - accuracy: 0.9723\nEpoch 20/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1218 - accuracy: 0.9685\nEpoch 21/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1163 - accuracy: 0.9692\nEpoch 22/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1203 - accuracy: 0.9707\nEpoch 23/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1256 - accuracy: 0.9685\nEpoch 24/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1355 - accuracy: 0.9654\nEpoch 25/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1179 - accuracy: 0.9710\n22/22 [==============================] - 0s 1ms/step - loss: 0.1463 - accuracy: 0.9648\nScore for Fold 7: loss of 0.14629295468330383; accuracy of 96.48093581199646%\n------------------------------------------------------------------------\nTraining Fold 8 / 10\nEpoch 1/25\n192/192 [==============================] - 1s 2ms/step - loss: 0.6694 - accuracy: 0.6582\nEpoch 2/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.2399 - accuracy: 0.9432\nEpoch 3/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1553 - accuracy: 0.9661\nEpoch 4/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1602 - accuracy: 0.9631\nEpoch 5/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1428 - accuracy: 0.9672\nEpoch 6/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1394 - accuracy: 0.9674\nEpoch 7/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1376 - accuracy: 0.9674\nEpoch 8/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1548 - accuracy: 0.9611\nEpoch 9/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1434 - accuracy: 0.9652\nEpoch 10/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1291 - accuracy: 0.9695\nEpoch 11/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1262 - accuracy: 0.9674\nEpoch 12/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1359 - accuracy: 0.9626\nEpoch 13/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1191 - accuracy: 0.9710\nEpoch 14/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1246 - accuracy: 0.9677\nEpoch 15/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1361 - accuracy: 0.9637\nEpoch 16/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1176 - accuracy: 0.9708\nEpoch 17/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1207 - accuracy: 0.9708\nEpoch 18/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1206 - accuracy: 0.9691\nEpoch 19/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1325 - accuracy: 0.9648\nEpoch 20/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1065 - accuracy: 0.9720\nEpoch 21/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1273 - accuracy: 0.9638\nEpoch 22/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1280 - accuracy: 0.9672\nEpoch 23/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1255 - accuracy: 0.9641\nEpoch 24/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1157 - accuracy: 0.9701\nEpoch 25/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1145 - accuracy: 0.9699\n22/22 [==============================] - 0s 1ms/step - loss: 0.1413 - accuracy: 0.9692\nScore for Fold 8: loss of 0.1412932276725769; accuracy of 96.92082405090332%\n------------------------------------------------------------------------\nTraining Fold 9 / 10\nEpoch 1/25\n192/192 [==============================] - 1s 2ms/step - loss: 0.6482 - accuracy: 0.6681\nEpoch 2/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.2196 - accuracy: 0.9557\nEpoch 3/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1588 - accuracy: 0.9665\nEpoch 4/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1404 - accuracy: 0.9679\nEpoch 5/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1300 - accuracy: 0.9703\nEpoch 6/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1506 - accuracy: 0.9644\nEpoch 7/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1439 - accuracy: 0.9655\nEpoch 8/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1243 - accuracy: 0.9707\nEpoch 9/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1502 - accuracy: 0.9644\nEpoch 10/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1436 - accuracy: 0.9659\nEpoch 11/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1256 - accuracy: 0.9705\nEpoch 12/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1389 - accuracy: 0.9667\nEpoch 13/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1248 - accuracy: 0.9694\nEpoch 14/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1381 - accuracy: 0.9671\nEpoch 15/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1271 - accuracy: 0.9678\nEpoch 16/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1497 - accuracy: 0.9610\nEpoch 17/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1365 - accuracy: 0.9649\nEpoch 18/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1251 - accuracy: 0.9687\nEpoch 19/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1340 - accuracy: 0.9662\nEpoch 20/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1275 - accuracy: 0.9664\nEpoch 21/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1167 - accuracy: 0.9708\nEpoch 22/25\n","name":"stdout"},{"output_type":"stream","text":"192/192 [==============================] - 0s 2ms/step - loss: 0.1171 - accuracy: 0.9705\nEpoch 23/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1271 - accuracy: 0.9696\nEpoch 24/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1243 - accuracy: 0.9682\nEpoch 25/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1036 - accuracy: 0.9728\n22/22 [==============================] - 0s 1ms/step - loss: 0.1386 - accuracy: 0.9663\nScore for Fold 9: loss of 0.13862299919128418; accuracy of 96.62756323814392%\n------------------------------------------------------------------------\nTraining Fold 10 / 10\nEpoch 1/25\n192/192 [==============================] - 1s 2ms/step - loss: 0.6680 - accuracy: 0.6805\nEpoch 2/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.2341 - accuracy: 0.9470\nEpoch 3/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1549 - accuracy: 0.9664\nEpoch 4/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1366 - accuracy: 0.9677\nEpoch 5/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1297 - accuracy: 0.9706\nEpoch 6/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1273 - accuracy: 0.9709\nEpoch 7/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1435 - accuracy: 0.9662\nEpoch 8/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1329 - accuracy: 0.9695\nEpoch 9/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1285 - accuracy: 0.9693\nEpoch 10/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1255 - accuracy: 0.9690\nEpoch 11/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1246 - accuracy: 0.9688\nEpoch 12/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1408 - accuracy: 0.9658\nEpoch 13/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1241 - accuracy: 0.9686\nEpoch 14/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1215 - accuracy: 0.9704\nEpoch 15/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1328 - accuracy: 0.9652\nEpoch 16/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1294 - accuracy: 0.9669\nEpoch 17/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1196 - accuracy: 0.9702\nEpoch 18/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1340 - accuracy: 0.9654\nEpoch 19/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1384 - accuracy: 0.9642\nEpoch 20/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1309 - accuracy: 0.9635\nEpoch 21/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1296 - accuracy: 0.9638\nEpoch 22/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1336 - accuracy: 0.9648\nEpoch 23/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1277 - accuracy: 0.9671\nEpoch 24/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1277 - accuracy: 0.9670\nEpoch 25/25\n192/192 [==============================] - 0s 2ms/step - loss: 0.1223 - accuracy: 0.9690\n22/22 [==============================] - 0s 1ms/step - loss: 0.1315 - accuracy: 0.9692\nScore for Fold 10: loss of 0.1315024346113205; accuracy of 96.91630005836487%\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Asthetics again!\nprint('------------------------------------------------------------------------')\nprint('Score per Fold')\nprint('------------------------------------------------------------------------')\n# Print for how many ever folds\nfor i in range(0, len(acc_per_fold)):\n    #print('------------------------------------------------------------------------')\n    print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\nprint('------------------------------------------------------------------------')\n# This'll give us a good idea of the leeway for our accuracy\nprint('Average scores for all folds:')\nprint(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\nprint(f'> Loss: {np.mean(loss_per_fold)}')\nprint('------------------------------------------------------------------------')\nlowAvg = (np.mean(acc_per_fold)) - (np.std(acc_per_fold))\nhighAvg = (np.mean(acc_per_fold)) + (np.std(acc_per_fold))\nlowAvg = (round(lowAvg, 2))\nhighAvg = (round(highAvg, 2))\nprint(f'> Expect anywhere from: {lowAvg}% - {highAvg}% model accuracy')","execution_count":33,"outputs":[{"output_type":"stream","text":"------------------------------------------------------------------------\nScore per Fold\n------------------------------------------------------------------------\n> Fold 1 - Loss: 0.11468835920095444 - Accuracy: 97.3607063293457%\n> Fold 2 - Loss: 0.14338241517543793 - Accuracy: 96.77419066429138%\n> Fold 3 - Loss: 0.11359000205993652 - Accuracy: 97.06745147705078%\n> Fold 4 - Loss: 0.13220366835594177 - Accuracy: 96.62756323814392%\n> Fold 5 - Loss: 0.14365985989570618 - Accuracy: 96.18768095970154%\n> Fold 6 - Loss: 0.14040081202983856 - Accuracy: 96.334308385849%\n> Fold 7 - Loss: 0.14629295468330383 - Accuracy: 96.48093581199646%\n> Fold 8 - Loss: 0.1412932276725769 - Accuracy: 96.92082405090332%\n> Fold 9 - Loss: 0.13862299919128418 - Accuracy: 96.62756323814392%\n> Fold 10 - Loss: 0.1315024346113205 - Accuracy: 96.91630005836487%\n------------------------------------------------------------------------\nAverage scores for all folds:\n> Accuracy: 96.72975242137909 (+- 0.3344307178376142)\n> Loss: 0.13456367328763008\n------------------------------------------------------------------------\n> Expect anywhere from: 96.4% - 97.06% model accuracy\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Nice! This avg acc tell's us our general for the entire dataset so that we aren't just getting 99% in one split & 20% in the other. Fun Stuff :)\\","execution_count":34,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inputs = np.concatenate((X_train, X_test), axis=0)\ntargets = np.concatenate((Y_train, Y_test), axis=0)\n\nacc_per_fold = []\nloss_per_fold = []\n\nfold_no = 1\nnum_folds = 25\n\nkfold = KFold(n_splits=num_folds, shuffle=True)\n\n# K-fold Cross Validation model evaluation\nfor train, test in kfold.split(inputs, targets):\n\n    # Cool model stuff (Did some off notebook tweaking to get the best model!)\n    model = Sequential()\n\n    model.add(Dense(64, activation='relu', input_shape=(95,)))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.25))\n\n    model.add(Dense(64, activation='relu')) \n    model.add(BatchNormalization())\n    model.add(Dropout(0.25))\n\n    model.add(Dense(1, activation='sigmoid'))\n    \n\n    checkpoint = ModelCheckpoint('BankruptcyModel.h5',\n                             monitor='val_loss',\n                             mode='min',\n                             save_best_only=True,\n                             verbose=1)\n    earlystop = EarlyStopping(monitor='val_loss',\n                          min_delta=0,\n                          patience=25,\n                          verbose=1,\n                          restore_best_weights=True\n                          )\n    reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n                              factor=0.2,\n                              patience=3,\n                              verbose=1,\n                              min_delta=0.0001)\n    # This will help with accuracy\n    callbacks = [earlystop,checkpoint,reduce_lr]\n    \n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n    # Asthetics\n    print('------------------------------------------------------------------------')\n    print(f'Training Fold {fold_no} / {num_folds}')\n\n    history = model.fit(inputs[train], targets[train],\n              epochs=25,\n              callbacks = callbacks,\n              verbose=1)\n\n    # Generalization metrics\n    scores = model.evaluate(inputs[test], targets[test], verbose=1)\n    \n    print(f'Score for Fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n    acc_per_fold.append(scores[1] * 100)\n    loss_per_fold.append(scores[0])\n\n    fold_no = fold_no + 1","execution_count":null,"outputs":[{"output_type":"stream","text":"------------------------------------------------------------------------\nTraining Fold 1 / 25\nEpoch 1/25\n205/205 [==============================] - 1s 2ms/step - loss: 0.6441 - accuracy: 0.6721\nEpoch 2/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.2165 - accuracy: 0.9521\nEpoch 3/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1662 - accuracy: 0.9632\nEpoch 4/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1459 - accuracy: 0.9674\nEpoch 5/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1314 - accuracy: 0.9692\nEpoch 6/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1339 - accuracy: 0.9682\nEpoch 7/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1504 - accuracy: 0.9636\nEpoch 8/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1298 - accuracy: 0.9690\nEpoch 9/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1452 - accuracy: 0.9632\nEpoch 10/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1241 - accuracy: 0.9693\nEpoch 11/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1324 - accuracy: 0.9683\nEpoch 12/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1374 - accuracy: 0.9650\nEpoch 13/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1378 - accuracy: 0.9659\nEpoch 14/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1291 - accuracy: 0.9682\nEpoch 15/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1216 - accuracy: 0.9687\nEpoch 16/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1203 - accuracy: 0.9706\nEpoch 17/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1264 - accuracy: 0.9690\nEpoch 18/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1112 - accuracy: 0.9724\nEpoch 19/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1281 - accuracy: 0.9675\nEpoch 20/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1174 - accuracy: 0.9684\nEpoch 21/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1284 - accuracy: 0.9669\nEpoch 22/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1330 - accuracy: 0.9645\nEpoch 23/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1151 - accuracy: 0.9702\nEpoch 24/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1208 - accuracy: 0.9685\nEpoch 25/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1150 - accuracy: 0.9703\n9/9 [==============================] - 0s 1ms/step - loss: 0.0827 - accuracy: 0.9744\nScore for Fold 1: loss of 0.08270473778247833; accuracy of 97.43589758872986%\n------------------------------------------------------------------------\nTraining Fold 2 / 25\nEpoch 1/25\n205/205 [==============================] - 1s 2ms/step - loss: 0.6408 - accuracy: 0.6625\nEpoch 2/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.2114 - accuracy: 0.9529\nEpoch 3/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1505 - accuracy: 0.9699\nEpoch 4/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1545 - accuracy: 0.9649\nEpoch 5/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1460 - accuracy: 0.9661\nEpoch 6/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1313 - accuracy: 0.9694\nEpoch 7/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1436 - accuracy: 0.9658\nEpoch 8/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1382 - accuracy: 0.9663\nEpoch 9/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1384 - accuracy: 0.9672\nEpoch 10/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1406 - accuracy: 0.9655\nEpoch 11/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1354 - accuracy: 0.9673\nEpoch 12/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1330 - accuracy: 0.9671\nEpoch 13/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1326 - accuracy: 0.9697\nEpoch 14/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1314 - accuracy: 0.9677\nEpoch 15/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1291 - accuracy: 0.9673\nEpoch 16/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1332 - accuracy: 0.9672\nEpoch 17/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1329 - accuracy: 0.9658\nEpoch 18/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1289 - accuracy: 0.9663\nEpoch 19/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1265 - accuracy: 0.9676\nEpoch 20/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1269 - accuracy: 0.9670\nEpoch 21/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1198 - accuracy: 0.9691\nEpoch 22/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1260 - accuracy: 0.9662\nEpoch 23/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1295 - accuracy: 0.9664\nEpoch 24/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1177 - accuracy: 0.9708\nEpoch 25/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1195 - accuracy: 0.9684\n9/9 [==============================] - 0s 1ms/step - loss: 0.0646 - accuracy: 0.9890\nScore for Fold 2: loss of 0.06459654867649078; accuracy of 98.90109896659851%\n------------------------------------------------------------------------\nTraining Fold 3 / 25\nEpoch 1/25\n205/205 [==============================] - 1s 2ms/step - loss: 0.6542 - accuracy: 0.6749\nEpoch 2/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.2210 - accuracy: 0.9552\nEpoch 3/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1639 - accuracy: 0.9636\nEpoch 4/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1538 - accuracy: 0.9664\nEpoch 5/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1354 - accuracy: 0.9698\nEpoch 6/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1368 - accuracy: 0.9687\nEpoch 7/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1316 - accuracy: 0.9675\nEpoch 8/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1328 - accuracy: 0.9678\nEpoch 9/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1305 - accuracy: 0.9691\nEpoch 10/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1413 - accuracy: 0.9654\nEpoch 11/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1337 - accuracy: 0.9670\nEpoch 12/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1231 - accuracy: 0.9709\nEpoch 13/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1378 - accuracy: 0.9664\nEpoch 14/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1287 - accuracy: 0.9682\nEpoch 15/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1379 - accuracy: 0.9656\nEpoch 16/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1179 - accuracy: 0.9704\nEpoch 17/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1346 - accuracy: 0.9670\nEpoch 18/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1198 - accuracy: 0.9703\nEpoch 19/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1201 - accuracy: 0.9702\nEpoch 20/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1294 - accuracy: 0.9670\nEpoch 21/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1200 - accuracy: 0.9695\nEpoch 22/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1152 - accuracy: 0.9708\nEpoch 23/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1165 - accuracy: 0.9713\nEpoch 24/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1201 - accuracy: 0.9673\nEpoch 25/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1184 - accuracy: 0.9669\n","name":"stdout"},{"output_type":"stream","text":"9/9 [==============================] - 0s 1ms/step - loss: 0.1709 - accuracy: 0.9560\nScore for Fold 3: loss of 0.17094926536083221; accuracy of 95.60439586639404%\n------------------------------------------------------------------------\nTraining Fold 4 / 25\nEpoch 1/25\n205/205 [==============================] - 1s 2ms/step - loss: 0.6501 - accuracy: 0.6610\nEpoch 2/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.2141 - accuracy: 0.9579\nEpoch 3/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1575 - accuracy: 0.9660\nEpoch 4/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1499 - accuracy: 0.9657\nEpoch 5/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1557 - accuracy: 0.9640\nEpoch 6/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1356 - accuracy: 0.9697\nEpoch 7/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1392 - accuracy: 0.9661\nEpoch 8/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1491 - accuracy: 0.9618\nEpoch 9/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1332 - accuracy: 0.9682\nEpoch 10/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1341 - accuracy: 0.9675\nEpoch 11/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1312 - accuracy: 0.9680\nEpoch 12/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1129 - accuracy: 0.9724\nEpoch 13/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1382 - accuracy: 0.9661\nEpoch 14/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1283 - accuracy: 0.9682\nEpoch 15/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1258 - accuracy: 0.9683\nEpoch 16/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1287 - accuracy: 0.9675\nEpoch 17/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1254 - accuracy: 0.9694\nEpoch 18/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1282 - accuracy: 0.9666\nEpoch 19/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1162 - accuracy: 0.9706\nEpoch 20/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1234 - accuracy: 0.9693\nEpoch 21/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1247 - accuracy: 0.9672\nEpoch 22/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1210 - accuracy: 0.9683\nEpoch 23/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1268 - accuracy: 0.9669\nEpoch 24/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1354 - accuracy: 0.9654\nEpoch 25/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1264 - accuracy: 0.9686\n9/9 [==============================] - 0s 1ms/step - loss: 0.1298 - accuracy: 0.9707\nScore for Fold 4: loss of 0.1298188865184784; accuracy of 97.0695972442627%\n------------------------------------------------------------------------\nTraining Fold 5 / 25\nEpoch 1/25\n205/205 [==============================] - 1s 2ms/step - loss: 0.6583 - accuracy: 0.6878\nEpoch 2/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.2191 - accuracy: 0.9533: 0s - loss: 0.2368 - accuracy\nEpoch 3/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1574 - accuracy: 0.9639\nEpoch 4/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1600 - accuracy: 0.9647\nEpoch 5/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1469 - accuracy: 0.9685\nEpoch 6/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1378 - accuracy: 0.9674\nEpoch 7/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1375 - accuracy: 0.9682\nEpoch 8/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1403 - accuracy: 0.9664\nEpoch 9/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1460 - accuracy: 0.9650\nEpoch 10/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1313 - accuracy: 0.9668\nEpoch 11/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1259 - accuracy: 0.9704\nEpoch 12/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1391 - accuracy: 0.9638\nEpoch 13/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1274 - accuracy: 0.9698\nEpoch 14/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1304 - accuracy: 0.9662\nEpoch 15/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1273 - accuracy: 0.9683\nEpoch 16/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1273 - accuracy: 0.9683\nEpoch 17/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1343 - accuracy: 0.9664\nEpoch 18/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1221 - accuracy: 0.9697\nEpoch 19/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1191 - accuracy: 0.9676\nEpoch 20/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1144 - accuracy: 0.9704\nEpoch 21/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1232 - accuracy: 0.9675\nEpoch 22/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1151 - accuracy: 0.9707\nEpoch 23/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1287 - accuracy: 0.9650\nEpoch 24/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1253 - accuracy: 0.9674\nEpoch 25/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1211 - accuracy: 0.9688\n9/9 [==============================] - 0s 1ms/step - loss: 0.2063 - accuracy: 0.9597\nScore for Fold 5: loss of 0.20627889037132263; accuracy of 95.9706962108612%\n------------------------------------------------------------------------\nTraining Fold 6 / 25\nEpoch 1/25\n205/205 [==============================] - 1s 2ms/step - loss: 0.6651 - accuracy: 0.6655\nEpoch 2/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.2204 - accuracy: 0.9495\nEpoch 3/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1506 - accuracy: 0.9677\nEpoch 4/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1532 - accuracy: 0.9650\nEpoch 5/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1479 - accuracy: 0.9660\nEpoch 6/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1455 - accuracy: 0.9647\nEpoch 7/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1367 - accuracy: 0.9684\nEpoch 8/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1246 - accuracy: 0.9707\nEpoch 9/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1373 - accuracy: 0.9678\nEpoch 10/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1422 - accuracy: 0.9645\nEpoch 11/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1394 - accuracy: 0.9645\nEpoch 12/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1261 - accuracy: 0.9671\nEpoch 13/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1332 - accuracy: 0.9676\nEpoch 14/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1388 - accuracy: 0.9637\nEpoch 15/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1325 - accuracy: 0.9666\nEpoch 16/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1153 - accuracy: 0.9728\nEpoch 17/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1269 - accuracy: 0.9661\nEpoch 18/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1247 - accuracy: 0.9679\nEpoch 19/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1367 - accuracy: 0.9642\nEpoch 20/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1245 - accuracy: 0.9693\nEpoch 21/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1237 - accuracy: 0.9698\nEpoch 22/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1267 - accuracy: 0.9665\nEpoch 23/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1201 - accuracy: 0.9713\nEpoch 24/25\n","name":"stdout"},{"output_type":"stream","text":"205/205 [==============================] - 0s 2ms/step - loss: 0.1272 - accuracy: 0.9688\nEpoch 25/25\n205/205 [==============================] - 0s 2ms/step - loss: 0.1142 - accuracy: 0.9718\n9/9 [==============================] - 0s 2ms/step - loss: 0.0727 - accuracy: 0.9817\nScore for Fold 6: loss of 0.07267742604017258; accuracy of 98.16849827766418%\n------------------------------------------------------------------------\nTraining Fold 7 / 25\nEpoch 1/25\n205/205 [==============================] - 1s 2ms/step - loss: 0.6410 - accuracy: 0.6793\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Asthetics again!\nprint('------------------------------------------------------------------------')\nprint('Score per Fold')\nprint('------------------------------------------------------------------------')\n# Print for how many ever folds\nfor i in range(0, len(acc_per_fold)):\n    #print('------------------------------------------------------------------------')\n    print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\nprint('------------------------------------------------------------------------')\n# This'll give us a good idea of the leeway for our accuracy\nprint('Average scores for all folds:')\nprint(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\nprint(f'> Loss: {np.mean(loss_per_fold)}')\nprint('------------------------------------------------------------------------')\nlowAvg = (np.mean(acc_per_fold)) - (np.std(acc_per_fold))\nhighAvg = (np.mean(acc_per_fold)) + (np.std(acc_per_fold))\nlowAvg = (round(lowAvg, 2))\nhighAvg = (round(highAvg, 2))\nprint(f'> Expect anywhere from: {lowAvg}% - {highAvg}% model accuracy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# And Voila! Increased fold number: Got better generalization, similar average as last time, with a wider range of accuracy :)\\","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}